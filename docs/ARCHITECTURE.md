# System Architecture

## Overview

The Food RAG Web Application is a full-stack AI-powered system that combines vector-based semantic search with Large Language Model (LLM) generation to provide intelligent responses about food-related queries.

## Architecture Evolution

This project evolved through three phases, each demonstrating significant architectural improvements:

### Phase 1: Local System (Week 2)
```
foods.json â†’ Ollama mxbai-embed-large â†’ ChromaDB â†’ Ollama llama3.2
   (90)         (LOCAL ~2.2s)           (LOCAL)      (LOCAL ~21s)
                        
              Average Response Time: 23,691ms (~24 seconds)
```

### Phase 2: Cloud Migration (Week 3)
```
foods.json â†’ Upstash Vector (auto-embed) â†’ Groq Cloud llama-3.1-8b
  (110)         (CLOUD ~259ms)                (CLOUD ~547ms)
                        
              Average Response Time: 806ms (<1 second)
                      ðŸš€ 29.4x FASTER! ðŸš€
```

### Phase 3: Web Application (Weeks 4-5)
```
Next.js UI â†’ Server Actions/API â†’ Upstash Vector â†’ Groq API â†’ Streaming UI
                                     
              Production-ready with real-time streaming responses
```

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER INTERFACE (Next.js)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Chat Input â”‚  â”‚  Response   â”‚  â”‚   Sources   â”‚  â”‚ Performance Metrics â”‚ â”‚
â”‚  â”‚   + Send    â”‚  â”‚   Display   â”‚  â”‚   Display   â”‚  â”‚     Dashboard       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚                â”‚                     â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                â”‚                     â”‚
          â–¼                â”‚                â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        APPLICATION LAYER                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     Next.js Server Actions                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ ragQuery()      â”‚            â”‚ /api/chat (Streaming)           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ - Non-streaming â”‚            â”‚ - Real-time text streaming      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ - Full response â”‚            â”‚ - Vercel AI SDK integration     â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚           â”‚                                      â”‚                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                      â”‚
               â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       VECTOR DATABASE            â”‚    â”‚          LLM SERVICE             â”‚
â”‚      (Upstash Vector)            â”‚    â”‚          (Groq API)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Semantic Search         â”‚  â”‚    â”‚  â”‚  â€¢ llama-3.1-8b-instant    â”‚  â”‚
â”‚  â”‚  â€¢ Text Embeddings         â”‚  â”‚    â”‚  â”‚  â€¢ Temperature: 0.7        â”‚  â”‚
â”‚  â”‚  â€¢ TopK: 3 Results         â”‚  â”‚    â”‚  â”‚  â€¢ Max Tokens: 500         â”‚  â”‚
â”‚  â”‚  â€¢ Metadata Storage        â”‚  â”‚    â”‚  â”‚  â€¢ Streaming Support       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚    â”‚                                  â”‚
â”‚  Data Structure:                 â”‚    â”‚  System Prompt:                  â”‚
â”‚  {                               â”‚    â”‚  "You are a helpful food         â”‚
â”‚    id: string,                   â”‚    â”‚   knowledge assistant..."        â”‚
â”‚    vector: number[],             â”‚    â”‚                                  â”‚
â”‚    metadata: {                   â”‚    â”‚  Context Window:                 â”‚
â”‚      text: string,               â”‚    â”‚  [1] Source 1 text...            â”‚
â”‚      category: string,           â”‚    â”‚  [2] Source 2 text...            â”‚
â”‚      origin: string              â”‚    â”‚  [3] Source 3 text...            â”‚
â”‚    }                             â”‚    â”‚                                  â”‚
â”‚  }                               â”‚    â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### 1. User Query Flow

```
User Input â†’ Next.js Client â†’ Server Action/API â†’ Vector Search â†’ LLM â†’ Response
```

### 2. Detailed Process

1. **User Input**: User types a food-related question
2. **Client Processing**: React component captures input and triggers request
3. **Server Action**: `ragQuery()` or `/api/chat` receives the question
4. **Vector Search**: 
   - Question converted to embeddings by Upstash
   - Semantic similarity search performed
   - Top 3 most relevant documents retrieved
5. **Context Building**: 
   - Retrieved documents formatted as numbered context
   - Metadata extracted (category, origin)
6. **LLM Processing**:
   - System prompt + context + question sent to Groq
   - LLM generates contextual response
7. **Response Delivery**:
   - Answer returned with sources and metrics
   - UI updates with response

## Component Architecture

### Frontend Components

```
components/
â”œâ”€â”€ food-chat.tsx       # Main chat interface
â”‚   â”œâ”€â”€ Message Display
â”‚   â”œâ”€â”€ Source Cards
â”‚   â”œâ”€â”€ Metrics Dashboard
â”‚   â”œâ”€â”€ Streaming Toggle
â”‚   â””â”€â”€ Share Buttons
â”œâ”€â”€ header.tsx          # Navigation & theme toggle
â”œâ”€â”€ footer.tsx          # Footer links
â”œâ”€â”€ particle-background.tsx  # Visual effects
â”œâ”€â”€ theme-provider.tsx  # Dark/light mode
â””â”€â”€ ui/                 # Shadcn components
```

### Backend Services

```
app/
â”œâ”€â”€ actions.ts          # Server action (non-streaming)
â”‚   â””â”€â”€ ragQuery()
â”œâ”€â”€ api/
â”‚   â””â”€â”€ chat/
â”‚       â””â”€â”€ route.ts    # Streaming API endpoint
â””â”€â”€ page.tsx            # Main page component
```

## Performance Metrics

The system tracks and displays:

| Metric | Description | Typical Range |
|--------|-------------|---------------|
| Vector Search Time | Time for Upstash query | 100-200ms |
| LLM Processing Time | Groq API response time | 500-1500ms |
| Total Response Time | End-to-end latency | 600-1700ms |
| Tokens Used | LLM tokens consumed | 200-600 |

## Streaming vs Non-Streaming

| Feature | Non-Streaming | Streaming |
|---------|---------------|-----------|
| Response Delivery | Full response at once | Character by character |
| Perceived Performance | Slower (wait for full) | Faster (immediate feedback) |
| Implementation | Server Action | API Route + AI SDK |
| User Experience | Simple | More engaging |

## Security Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vercel Edge                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Environment Variables             â”‚  â”‚
â”‚  â”‚  â€¢ UPSTASH_VECTOR_REST_URL                    â”‚  â”‚
â”‚  â”‚  â€¢ UPSTASH_VECTOR_REST_TOKEN                  â”‚  â”‚
â”‚  â”‚  â€¢ GROQ_API_KEY                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                            â”‚
â”‚                    Server-Side                       â”‚
â”‚                    API Calls                         â”‚
â”‚                         â”‚                            â”‚
â”‚                         â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               Client (Browser)                 â”‚  â”‚
â”‚  â”‚        No API keys or tokens exposed          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Technology Stack Summary

| Layer | Technology | Purpose |
|-------|------------|---------|
| Frontend | Next.js 15, React 19 | UI framework |
| Styling | Tailwind CSS, Shadcn UI | Design system |
| Backend | Next.js Server Actions | API logic |
| Vector DB | Upstash Vector | Semantic search |
| LLM | Groq (llama-3.1-8b) | Response generation |
| Streaming | Vercel AI SDK | Real-time responses |
| Deployment | Vercel | Hosting & edge |
| State | localStorage | Chat persistence |

## Performance Comparison: Local vs Cloud

The migration from local to cloud infrastructure achieved significant performance improvements:

| Metric | Local (Week 2) | Cloud (Week 3) | Improvement |
|--------|----------------|----------------|-------------|
| Embedding + Retrieval | 2,196ms | 259ms | **+88.2%** |
| LLM Generation | 21,493ms | 547ms | **+97.5%** |
| **Total Response** | **23,691ms** | **806ms** | **+96.6%** |

### Key Migration Changes

| Component | Before (Local) | After (Cloud) |
|-----------|---------------|---------------|
| **Vector Database** | ChromaDB (local SQLite) | Upstash Vector (serverless) |
| **Embeddings** | Ollama mxbai-embed-large (manual) | Upstash built-in (automatic) |
| **LLM** | Ollama llama3.2 (local) | Groq llama-3.1-8b-instant (cloud) |
| **Food Database** | 90 items | 110 items |
| **Infrastructure** | Local machine required | Serverless, auto-scaling |

For detailed migration documentation, see [python-reference/docs/MIGRATION_PLAN.md](../python-reference/docs/MIGRATION_PLAN.md).
